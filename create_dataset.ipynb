{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609e5e03",
   "metadata": {},
   "source": [
    "Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a48d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eca26d",
   "metadata": {},
   "source": [
    "Скачивание и сохранения датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем CIFAR-100 из Keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Создадим базовую папку для датасета\n",
    "base_dir = \"cifar100_raw\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Сохраняем в виде изображений по классам\n",
    "def save_dataset(images, labels, split):\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "        class_id = int(label[0])\n",
    "        class_dir = os.path.join(split_dir, str(class_id))\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        im = Image.fromarray(img)\n",
    "        im.save(os.path.join(class_dir, f\"{idx}.png\"))\n",
    "\n",
    "save_dataset(x_train, y_train, \"train\")\n",
    "save_dataset(x_test, y_test, \"test\")\n",
    "\n",
    "print(\"Датасет сохранён в папку cifar100_raw/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855dbfdd",
   "metadata": {},
   "source": [
    "Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Генератор с аугментациями\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,       # случайные повороты\n",
    "    width_shift_range=0.1,   # сдвиги по ширине\n",
    "    height_shift_range=0.1,  # сдвиги по высоте\n",
    "    horizontal_flip=True,    # отражения\n",
    "    zoom_range=0.1           # масштабирование\n",
    ")\n",
    "\n",
    "# Папка для сохранения аугментированных данных\n",
    "aug_dir = \"cifar100_augmented\"\n",
    "os.makedirs(aug_dir, exist_ok=True)\n",
    "\n",
    "def augment_and_save(split=\"train\", samples_per_image=2):\n",
    "    src_dir = os.path.join(base_dir, split)\n",
    "    for class_id in os.listdir(src_dir):\n",
    "        class_src = os.path.join(src_dir, class_id)\n",
    "        class_dst = os.path.join(aug_dir, split, class_id)\n",
    "        os.makedirs(class_dst, exist_ok=True)\n",
    "\n",
    "        for img_name in os.listdir(class_src):\n",
    "            img_path = os.path.join(class_src, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            x = np.expand_dims(np.array(img), axis=0)  # (1,h,w,3)\n",
    "\n",
    "            # создаём N аугментированных версий\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x, batch_size=1):\n",
    "                im = Image.fromarray(batch[0].astype(\"uint8\"))\n",
    "                im.save(os.path.join(class_dst, f\"{img_name[:-4]}_aug{i}.png\"))\n",
    "                i += 1\n",
    "                if i >= samples_per_image:  # сколько копий на одно изображение\n",
    "                    break\n",
    "\n",
    "augment_and_save(\"train\", samples_per_image=2)\n",
    "print(\"Аугментация сохранена в cifar100_augmented/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdd8fa",
   "metadata": {},
   "source": [
    "Обучение модели EfficientNetV2B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41b1890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Num GPUs Available: 0\n",
      "GPU list: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU list:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4335dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 100 classes.\n",
      "Found 10000 files belonging to 100 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m2363/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 128ms/step - accuracy: 0.3783 - loss: 2.5722"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "\n",
    "img_size = (224,224)  # EfficientNet требует увеличенных изображений\n",
    "batch_size = 16\n",
    "num_classes = 100\n",
    "\n",
    "# Загружаем данные из сохранённой структуры директорий\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"cifar100_augmented/train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"cifar100_raw/test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Кеширование и prefetch\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Data augmentation (runtime, для доп.разнообразия)\n",
    "data_augmentation = models.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Базовая модель EfficientNetV2B0\n",
    "base_model = EfficientNetV2B0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "base_model.trainable = False  # замораживаем backbone\n",
    "\n",
    "# Строим модель\n",
    "inputs = layers.Input(shape=(224,224,3))\n",
    "x = data_augmentation(inputs)\n",
    "x = tf.keras.applications.efficientnet_v2.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Компиляция\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Обучение (первый этап, только классификатор)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Fine-tuning (разморозим backbone)\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:  # разморозим только верхние 50 слоёв\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad130a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
